{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KV cache要注意动态mask，预填充阶段才会有真正的mask，解码阶段q每次就一个token，不需要mask。\n",
    "添加KV cache的关键思路就是在模型输出阶段同时输出KV cache，然后解码阶段输入q的同时输入KV cache，在计算k和v的时候根据q计算一个token然后再与之前的KV cache拼接起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PositionalEncoding 位置编码类\n",
    "1. 初始化 传入参数d_model, max_len\n",
    "   1. 父类初始化\n",
    "   2. 参数初始化 self.d_model\n",
    "   3. 初始化位置编码（max_len, d_model）和位置张量(max_len, 1)\n",
    "   4. 计算位置编码分母 div (d_model/2, 1) $10000^{2i/d_model}$\n",
    "   5. 计算位置编码pe，按奇偶分别计算\n",
    "   6. pe増维(1, max_len, d_model)\n",
    "   7. 给pe设置持久缓存\n",
    "2. 前向 传入输入 x\n",
    "   1. 放大x防止被位置编码淹没\n",
    "   2. x + pe 注意pe要根据x的序列长度截断第二维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        \"\"\"\n",
    "        为序列加入位置编码\n",
    "        Args:\n",
    "            d_model: 序列矩阵的embedding的维度\n",
    "            max_len: 位置编码矩阵的最大序列长度, 这个长度可以比实际序列长度长, 相加时只要截取实际序列的长度即可\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)  # 创建一个(max_len, d_model)的全零矩阵, 用于保存位置编码值\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # 创建一个(max_len, 1)的矩阵, 表示位置索引\n",
    "        \n",
    "        # 创建一个(d_model/2,)的矩阵, 用于储存每个维度的频率因子(每两列的频率因子是相同的, 因此一共有d_model/2个频率因子)\n",
    "        # torch.arange(0, d_model, 2).float()相当于生成位置编码公式中的索引i\n",
    "        # 使用log和exp分开计算能够确保在数值范围内进行线性缩放, 从而避免浮点数溢出或精度丢失\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 计算位置编码\n",
    "        # 对于维度的偶数列\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 由广播机制：(max_len, 1)*(d_model/2,)->(max_len, d_model/2)\n",
    "        # 对于维度的奇数列\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # 增加一个batch维度, 使其能够与输入张量相加\n",
    "        pe = pe.unsqueeze(0)  # (max_len, d_model)->(1, max_len, d_model)\n",
    "        # 将位置编码矩阵注册为模型的缓冲区, 这样它将不会被认为是模型的参数\n",
    "        # 缓冲区会随着模型一起保存和加载\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_len, d_model)\n",
    "        output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 原文3.4节中提到, 为了使得单词嵌入表示相对大一些, 乘sqrt(d_model), 以确保嵌入向量的值不会被位置编码淹没。\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 将位置编码添加到输入张量上\n",
    "        # 位置编码依据max_len生成, 而输入序列长度的seq_len应小于等于max_len\n",
    "        # 通常会将输入序列补全或截断到统一长度, 让这个长度等于max_len即可\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x *= math.sqrt(self.d_model)\n",
    "        x += self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "pe = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "x = torch.randn(32, 50, d_model)\n",
    "\n",
    "x = pe(x)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "pe_matrix = pe.pe[0]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_matrix.detach().numpy(), aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Positional Encoding Matrix')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHeadAttention 多头注意力类\n",
    "1. 初始化 传入参数 d_model, num_heads, dropout\n",
    "   1. 父类初始化\n",
    "   2. 检查d_model 能否完整分为num_heads个头\n",
    "   3. 计算q, k, v的维度 d_model // num_heads, d_v可以与前两者不一样，默认取一样\n",
    "   4. 初始化参数 d_model, num_heads, d_k\n",
    "   5. 定义网络层w_q, w_k, w_v, w_o, dropout, 四个线性层维度默认是(d_model, d_model)\n",
    "2. 前向传播 传入输入 q, k, v, mask=None\n",
    "   1. 获取batch_size x.size(0)\n",
    "   2. 计算q, k, v\n",
    "      1. 经过对应的线性层(batch_size, seq_len, d_model)\n",
    "      2. 调整形状为(batch_size, seq_len, num_heads, d_k)\n",
    "      3. 继续调整方便不同头分开计算(batch_size, num_heads, seq_len, d_k)\n",
    "   3. 计算q, k的注意力分数\n",
    "      1. 计算内积，注意k转置和内积的缩放\n",
    "      2. 如果有mask，scores = scores.masked_fill(mask == 0, -1e9)\n",
    "      3. 对最后一维计算softmax\n",
    "      4. dropout，得到Attention(batch_size, num_heads, seq_len, seq_len)\n",
    "   4. 计算输出\n",
    "      1. 注意力分数与v进行内积 output(batch_size, num_heads, seq_len, d_k)\n",
    "      2. 调整输出形状(batch_size, seq_len, num_heads, d_k)\n",
    "      3. 设置张量连续存储 contiguous()\n",
    "      4. 合并维度 (batch_size, seq_len, d_model)\n",
    "      5. 经过线性层w_o\n",
    "   5. 返回输出和注意力\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头自注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"  # 确保num_heads能整除d_model\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads  # 这里简单起见，我们只考虑 d_v = d_k = d_q = d_model / num_heads，因此只定义d_k\n",
    "        self.h = num_heads\n",
    "\n",
    "        # 这里定义的 linear 参数是 (d_model, d_model)\n",
    "        self.q_linear = nn.Linear(d_model, d_model)  # W_Q\n",
    "        self.k_linear = nn.Linear(d_model, d_model)  # W_K\n",
    "        self.v_linear = nn.Linear(d_model, d_model)  # W_V\n",
    "        self.o_linear = nn.Linear(d_model, d_model)  # W_O\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            q, k, v: (batch_size, seq_len, d_model)\n",
    "                对于自注意力, 如果输入序列为 x, 那么 q=x, k=x, v=x\n",
    "                对于交叉注意力, 如果序列 x_1 对序列 x_2 做 query, 则 q=x_1, k=x_2, v=x_2\n",
    "            mask: (batch_size, 1, 1, seq_len)或(batch_size, 1, seq_len, seq_len)\n",
    "                mask有多种形式, 可以使用0、1来mask, 也可以使用True、False来mask, 根据具体代码执行mask\n",
    "        output:\n",
    "            seq: (batch_size, seq_len, d_model)\n",
    "            attention: (batch_size, h, len_q, len_k) 每个头均有一个注意力权重矩阵\n",
    "                对于自注意力, len_q = len_k = len_v = seq_len\n",
    "                对于交叉注意力, len_q = tgt_seq_len , len_k = len_v = src_seq_len\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 将原始序列变换为QKV矩阵\n",
    "        # 以 q 的变换为例。序列 q=x 经过 q_linear 变换后，形状仍然为(batch_size, seq_len, d_model)\n",
    "        # 使用.view方法用于改变张量形状。这里变换成了(batch_size, seq_len, num_heads, d_k)，即把 d_model 拆成了 num_heads*d_k\n",
    "        # 使用.transpose方法，将形状进一步变为(batch_size, num_heads, seq_len, d_k)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)  # (batch_size, seq_len, d_model)->(batch_size, num_heads, seq_len, d_k)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 每个头并行计算相似度得分，相似度矩阵形状为(batch_size, num_heads, len_q, len_k)\n",
    "        # 即每个头都形成了(len_q, len_k)的 scores，scores 的第一行，意思是第一个位置的 q 对所有位置的 k 的得分，因此后续的 softmax 是按 scores 的行来做的\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k) # 默认乘最后两个维度的矩阵\n",
    "        \n",
    "        if mask is not None:\n",
    "            # 这里我们假设mask中为0的地方是需要遮蔽的地方\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)  # 通过把掩码的位置设置为一个较大的负数，让掩码位置的softmax趋近于零\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)  # 得到所有batch的每个头的相似度矩阵\n",
    "        \n",
    "        # 相似度矩阵与v相乘得到输出\n",
    "        output = torch.matmul(attention, v)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 首先将output变为(batch_size, seq_len, num_heads, d_k)\n",
    "        # .contiguous用于确保张量在内存中是连续的\n",
    "        # 将张量形状变为(batch_size, seq_len, d_model)，相当于把所有头的结果拼接了起来，即 d_k*num_heads 拼成了 d_model\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.o_linear(output)  # 使用w_o进行线性变换\n",
    "        \n",
    "        # 最终传出输出和每个头的attention，attention根据需要可用于后续的可视化\n",
    "        return output, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None, kv_cache=None):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            cached_k, cacahe_v = kv_cache\n",
    "            k = torch.cat([cached_k, k], dim=-2)\n",
    "            v = torch.cat([cacahe_v, v], dim=-2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            if scores.shape[-1] != mask.shape[-1]:\n",
    "                pass\n",
    "            scores = scores.masked_fill(mask==0, 1e-9)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        output = torch.matmul(attention, v)\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.w_o(output)\n",
    "\n",
    "        return output, attention, (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.h = num_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.o_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, kv_cache=None):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        if kv_cache is not None:\n",
    "            k_cache, v_cache = kv_cache\n",
    "            k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            k = torch.cat([k_cache, k], dim=-2)\n",
    "            v = torch.cat([v_cache, v], dim=-2)\n",
    "        else:\n",
    "            k = self.k_linear(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            v = self.v_linear(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        output = torch.matmul(attention, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.o_linear(output)\n",
    "        return output, attention, (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "\n",
    "x = torch.randn(32, 10, d_model)\n",
    "\n",
    "output, attention, kv_cache = mha(x, x, x)\n",
    "\n",
    "print(output.shape)\n",
    "print(attention.shape)\n",
    "print(kv_cache[0].shape)\n",
    "\n",
    "# 可视化第0个batch的第0个头的attention\n",
    "head_attention = attention[0, 0].detach().numpy()  # 提取第 0 个 batch 的第 0 个头\n",
    "\n",
    "# 绘制热力图\n",
    "plt.imshow(head_attention, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Attention Weights')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FeedForward 前馈神经网络 通常在注意力层之后\n",
    "1. 初始化 传入参数 d_model, d_ff, dropout\n",
    "   1. 父类初始化\n",
    "   2. 定义升维线性层(d_model, d_ff)\n",
    "   3. 定义dropout\n",
    "   4. 定义降维线性层(d_model, d_ff)\n",
    "2. 前向传播 传入参数 x\n",
    "   1. 经过第一次线性层并计算relu激活\n",
    "   2. 经过dropout\n",
    "   3. 经过第二层线性层\n",
    "   4. 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # d_ff 默认设置为 2048，更多的中间层节点数可以增加网络的容量，使其能够学习更复杂的函数映射。\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_len, d_model)\n",
    "        output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.w_1(x)))\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LayerNorm 层归一化类 对每个样本的最后一位\n",
    "1. 初始化 传入 d_model, eps\n",
    "   1. 父类初始化\n",
    "   2. 初始化缩放(1)和平移参数(0)(d_model,),要定义为可训练的参数\n",
    "   3. 初始化epsilon\n",
    "2. 前向传播 传入输入 x\n",
    "   1. 针对输入的最后一维(d_model)计算均值和方差\n",
    "   2. 归一化后进行缩放和偏移\n",
    "   3. 返回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 层归一化, 也可以使用PyTorch内置的层归一化nn.LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a = nn.Parameter(torch.ones(d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LayerNorm是对d_model而言的\n",
    "        mean = x.mean(-1, keepdim=True)  # (batch_size, seq_len, 1)\n",
    "        std = x.std(-1, keepdim=True)  # (batch_size, seq_len, 1)\n",
    "        return self.a * (x - mean) / (std + self.eps) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.ones(d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a*((x - mean)/(std + self.eps)) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EncoderLayer 编码器层类\n",
    "1. 初始化 传入 d_model, num_heads, d_ff, dropout\n",
    "   1. 父类初始化\n",
    "   2. 定义两个层归一化层，分别用子多头注意力和前向的残差连接之后\n",
    "   3. 定义多头注意力层\n",
    "   4. 定义前向层\n",
    "   5. 定义两个dropout分别用于多头注意力和前向之后\n",
    "2. 前向 传入 x, src_mask=None\n",
    "   1. 计算注意力后的输出\n",
    "   2. 对输出进行dropout，然后连接残差，最后进行LayerNorm\n",
    "   3. 继续经过前向网络，经过dropout，进行残差连接，最后LayerNorm\n",
    "   4. 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        每个EncoderLayer包括两个子层: 多头注意力层和前馈神经网络层。每个子层都使用了残差连接和层归一化。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff=d_ff, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        \"\"\"\n",
    "        原文中使用: LayerNorm(x + SubLayer(x))\n",
    "        也有部分实现使用: x + SubLayer(LayerNorm(x))\n",
    "        这里我们使用原文的实现\n",
    "        input: (batch_size, seq_len, d_model)\n",
    "        output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        output, _ = self.attn(x, x, x, mask=src_mask)\n",
    "        x = self.norm_1(x + self.dropout_1(output))  # 多头自注意力子层\n",
    "        x = self.norm_2(x + self.dropout_2(self.ff(x)))  # 前馈神经网络子层\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, src_mask=None):\n",
    "        attn, _, kv = self.attn(x, x, x, mask = src_mask)\n",
    "        x = self.norm_1(x + self.dropout_1(attn))\n",
    "        x = self.norm_2(x + self.dropout_2(self.ffn(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder 编码器类 叠加多个编码器层\n",
    "1. 初始化 传入 num_layers, d_model, num_heads, d_ff, dropout\n",
    "   1. 父类初始化\n",
    "   2. 定义多个编码器层\n",
    "   3. 定义层归一化层加在整个编码器后\n",
    "2. 前向 传入 x, src_mask\n",
    "   1. 依次经过每个编码器层\n",
    "   2. 最后经过层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    编码器由多个编码器层堆叠而成。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        在原始论文的图 1 和描述中, 作者提到每个子层(Multi-Head Attention 和 Feed-Forward Network)之后会进行 Layer Normalization。\n",
    "        但是，论文并没有明确提到在整个编码器或解码器之后进行额外的 Layer Normalization。\n",
    "        许多后续的实现，通常会在编码器和解码器的堆叠之后再进行一次 Layer Normalization。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_len, d_model)\n",
    "        output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = LayerNorm(d_model)\n",
    "    def forward(self, x, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecoderLayer 解码器层类\n",
    "1. 初始化 传入 d_model, num_heads, d_ff. dropout\n",
    "   1. 父类初始化\n",
    "   2. 定义三个层归一化层，分别加在自注意力、交叉注意力和前馈层的残差连接后\n",
    "   3. 定义自注意力层\n",
    "   4. 定义交叉注意力层\n",
    "   5. 定义前馈层\n",
    "   6. 定义三个dropout层分别加载三个层的后面 ？一个\n",
    "2. 前向 传入 x, enc_output, memory_mask, tgt_mask\n",
    "   1. 对输入x计算自注意力attn，注意tgt_mask\n",
    "   2. dropout，残差连接，层归一化\n",
    "   3. 计算x和编码器输出的交叉注意力，注意传参，x是q, enc_mask\n",
    "   4. dropout，残差连接，层归一化\n",
    "   5. 经过前向，dropout，残差连接，层归一化\n",
    "   6. 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        每个DecoderLayer包括三个子层: 自注意力层、编码器-解码器注意力层和前馈神经网络层。每个子层都使用了残差连接和层归一化。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff=d_ff, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, memory_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_len, d_model)\n",
    "        output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        output_1, _ = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.norm_1(x + self.dropout_1(output_1))  # 第一个子层：多头自注意力层\n",
    "        \n",
    "        output_2, _ = self.enc_dec_attn(x, enc_output, enc_output, mask=memory_mask)  # k, v来自编码器输出\n",
    "        x = self.norm_2(x + self.dropout_2(output_2))  # 第二个子层：编码器-解码器注意力层\n",
    "        \n",
    "        x = self.norm_3(x + self.dropout_3(self.ff(x)))  # 第三个子层：前馈神经网络层\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.norm_3 = LayerNorm(d_model)\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, memory_mask=None, tgt_mask=None, kv_cache_self=None, kv_cache_enc_dec=None):\n",
    "\n",
    "        output_1, _, kv_cache_self = self.self_attn(x, x, x, mask=tgt_mask, kv_cache=kv_cache_self)\n",
    "        x = self.norm_1(x + self.dropout(output_1))  \n",
    "        \n",
    "        output_2, _, kv_cache_enc_dec = self.enc_dec_attn(x, enc_output, enc_output, mask=memory_mask, kv_cache=kv_cache_enc_dec)  # k, v来自编码器输出\n",
    "        x = self.norm_2(x + self.dropout(output_2))  \n",
    "        x = self.norm_3(x + self.dropout(self.ff(x)))  \n",
    "        return x, kv_cache_self, kv_cache_enc_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder 解码器类\n",
    "1. 初始化 传入 num_layers, d_model, num_heads, d_ff, dropout\n",
    "   1. 父类初始化\n",
    "   2. 定义num_layers个解码器层\n",
    "   3. 定义一个层归一化层用于叠加后的解码器层\n",
    "2. 前向 传入 x, enc_output, memory_mask, tgt_mask\n",
    "   1. 逐层传播\n",
    "   2. 层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    解码器由多个解码器层堆叠而成。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_output, memory_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        input: (batch_size, seq_len, d_model)\n",
    "        output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, memory_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = LayerNorm(d_model)\n",
    "    def forward(self, x, enc_output, memory_mask=None, tgt_mask=None, kv_cache=None):\n",
    "        if kv_cache is None:\n",
    "            kv_cache = [None] * len(self.layers) * 2\n",
    "        new_kv_cache = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x, kv_cache_self, kv_cache_enc_dec = layer(x, enc_output, memory_mask, tgt_mask, kv_cache[2 * i], kv_cache[2 * i + 1])\n",
    "            new_kv_cache.extend([kv_cache_self, kv_cache_enc_dec])\n",
    "        return self.norm(x), new_kv_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 完整的类\n",
    "1. 初始化 传入 src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len\n",
    "   1. 父类初始化\n",
    "   2. 定义源嵌入层，将序列的词表维度转换成d_model\n",
    "   3. 定义目标嵌入层，将序列的词表维度转换成d_model\n",
    "   4. 定义位置编码层，用于给输入添加位置编码\n",
    "   5. 定义Encoder\n",
    "   6. 定义Decoder\n",
    "   7. 定义输出前向层\n",
    "   8. 定义dropout\n",
    "2. 前向 传入 src, tgt, src_mask, tgt_mask, memory_mask\n",
    "   1. 对于源序列和目标序列分别进行，嵌入，增加位置编码，dropout\n",
    "   2. 上述源序列的结果经过Encoder\n",
    "   3. Encoder的输出和上述目标序列的结果经过Decoder\n",
    "   4. 上述输出经过前向层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_layers=6, num_heads=8, d_ff=2048, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        # src_vocab_size和tgt_vocab_size分别是源序列和目标序列的词典大小\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)  # 定义嵌入层，用于将序列转换为维度为d_model的嵌入向量\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)  # 位置编码层\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        src和tgt为token_id\n",
    "        src: (batch_size, src_seq_len)\n",
    "        tgt: (batch_size, tgt_seq_len)\n",
    "        在 Transformer 模型中, 输入序列通常已经经过填充(padding)处理。\n",
    "        填充是为了使所有输入序列的长度一致，从而可以将它们放入一个批次中进行处理。\n",
    "        \"\"\"\n",
    "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))  # 位置编码后使用了dropout，原文在Regularization中有提到\n",
    "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, memory_mask, tgt_mask)\n",
    "        \n",
    "        # 在训练过程中，logits 通常会通过 CrossEntropyLoss 来计算损失，而 CrossEntropyLoss 会在内部应用 softmax\n",
    "        # 因此这里可以不用softmax，在推理阶段，可以在output后手动加入softmax\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import ne\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_layers=6,  num_heads=8, d_ff=2048, dropout=0.1, max_len=500):\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, kv_cache=None):\n",
    "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
    "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
    "\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output, new_kv_cache = self.decoder(tgt, enc_output, memory_mask, tgt_mask, kv_cache)\n",
    "\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output, new_kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充掩码\n",
    "def make_padding_mask(seq, pad_id, return_int=True, true_to_mask=False):\n",
    "    \"\"\"\n",
    "    构造padding mask, 参数设置根据不同的Transformer实现来确定\n",
    "    Args:\n",
    "        seq: 需要构造mask的序列(batch, seq_len), 该序列使还未进行Embedding, 里面放的是token_id\n",
    "        pad_id: 用于填充的特殊字符<PAD>所对应的token_id, 根据不同代码设置\n",
    "        return_int: 是否返回int形式的mask, 默认为True\n",
    "        true_to_mask: 默认为False, 对于bool mask: True代表在True的位置遮蔽, False代表在False的位置遮蔽。对于int mask: True代表在1的位置遮蔽, False代表在0的位置遮蔽\n",
    "    \n",
    "    Returns:\n",
    "        mask: (batch, seq_len), 不同的Transformer实现需输入的形状也不同, 根据需要进行后续更改\n",
    "    \"\"\"\n",
    "    mask = (seq == pad_id)  # (batch, seq_len), 在<PAD>的位置上生成True, 真实序列的位置为False\n",
    "\n",
    "    if true_to_mask is False:\n",
    "        mask = ~mask\n",
    "    \n",
    "    if return_int:\n",
    "        mask = mask.int()\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# 因果掩码\n",
    "def make_sequence_mask(seq, return_int=True, true_to_mask=False):\n",
    "    \"\"\"\n",
    "    构造sequence mask, 参数设置根据不同的Transformer实现来确定\n",
    "    Args:\n",
    "        seq: 需要构造mask的序列(batch, seq_len), 该序列使还未进行Embedding, 里面放的是token_id\n",
    "        return_int: 是否返回int形式的mask, 默认为True\n",
    "        true_to_mask: 默认为False, 对于bool mask: True代表在True的位置遮蔽, False代表在False的位置遮蔽。对于int mask: True代表在1的位置遮蔽, False代表在0的位置遮蔽\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len), 不同的Transformer实现需输入的形状也不同, 根据需要进行后续更改\n",
    "    \"\"\"\n",
    "    _, seq_len = seq.shape\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))  # (seq_len, seq_len), 下三角为1, 上三角为0\n",
    "    mask = 1 - mask\n",
    "    mask = mask.bool()\n",
    "\n",
    "    if true_to_mask is False:\n",
    "        mask = ~mask\n",
    "    \n",
    "    if return_int:\n",
    "        mask = mask.int()\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进一步分别构造src_mask、memory_mask、tgt_mask\n",
    "def make_src_mask(src, pad_id, return_int=True, true_to_mask=False):\n",
    "    \"\"\"构造src_mask\n",
    "\n",
    "    Args:\n",
    "        src: 源序列(batch_size, src_len)\n",
    "        pad_id: 补全符号的token_id\n",
    "        return_int: 是否返回int形式的mask, 默认为True\n",
    "        true_to_mask: 默认为False, 对于bool mask: True代表在True的位置遮蔽, False代表在False的位置遮蔽。对于int mask: True代表在1的位置遮蔽, False代表在0的位置遮蔽\n",
    "\n",
    "    Returns:\n",
    "        src_mask: (batch_size, 1, 1, src_len)\n",
    "    \"\"\"\n",
    "    padding_mask = make_padding_mask(src, pad_id, return_int=return_int, true_to_mask=true_to_mask)\n",
    "    padding_mask = padding_mask.unsqueeze(1)\n",
    "    padding_mask = padding_mask.unsqueeze(2)\n",
    "    return padding_mask\n",
    "\n",
    "def make_memory_mask(src, pad_id, return_int=True, true_to_mask=False):\n",
    "    \"\"\"构造memory_mask\n",
    "\n",
    "    Args:\n",
    "        src: 源序列(batch_size, src_len)\n",
    "        pad_id: 补全符号的token_id\n",
    "        return_int: 是否返回int形式的mask, 默认为True\n",
    "        true_to_mask: 默认为False, 对于bool mask: True代表在True的位置遮蔽, False代表在False的位置遮蔽。对于int mask: True代表在1的位置遮蔽, False代表在0的位置遮蔽\n",
    "\n",
    "    Returns:\n",
    "        memory_mask: (batch_size, 1, 1, src_len)\n",
    "    \"\"\"\n",
    "    padding_mask = make_padding_mask(src, pad_id, return_int=return_int, true_to_mask=true_to_mask)\n",
    "    padding_mask = padding_mask.unsqueeze(1)\n",
    "    padding_mask = padding_mask.unsqueeze(2)\n",
    "    return padding_mask\n",
    "\n",
    "def make_tgt_mask(tgt, pad_id, return_int=True, true_to_mask=False):\n",
    "    \"\"\"构造tgt_mask\n",
    "\n",
    "    Args:\n",
    "        tgt: 目标序列(batch_size, tgt_len)\n",
    "        pad_id: 补全符号的token_id\n",
    "        return_int: 是否返回int形式的mask, 默认为True\n",
    "        true_to_mask: 默认为False, 对于bool mask: True代表在True的位置遮蔽, False代表在False的位置遮蔽。对于int mask: True代表在1的位置遮蔽, False代表在0的位置遮蔽\n",
    "\n",
    "    Returns:\n",
    "        tgt_mask: (batch_size, 1, tgt_len, tgt_len)\n",
    "    \"\"\"\n",
    "    padding_mask = make_padding_mask(tgt, pad_id, return_int=return_int, true_to_mask=true_to_mask)  # (batch_size, tgt_len)\n",
    "    padding_mask = padding_mask.unsqueeze(1)\n",
    "    padding_mask = padding_mask.unsqueeze(2)  # (batch_size, 1, 1, tgt_len)\n",
    "    padding_mask = padding_mask.repeat(1, 1, tgt.size(1), 1)  # (batch_size, 1, tgt_len, tgt_len)\n",
    "\n",
    "    sequence_mask = make_sequence_mask(tgt, return_int=True, true_to_mask=False)  # (tgt_len, tgt_len)\n",
    "    sequence_mask = sequence_mask.unsqueeze(0)\n",
    "    sequence_mask = sequence_mask.unsqueeze(1)  # (1, 1, tgt_len, tgt_len)\n",
    "    sequence_mask = sequence_mask.repeat(tgt.size(0), 1, 1, 1)  # (batch_size, 1, tgt_len, tgt_len)\n",
    "\n",
    "    # 合并两个mask\n",
    "    if true_to_mask is False:  # 根据不同类型的mask, 使用\"与\"或\"或\"的方式进行合并\n",
    "        mask = padding_mask & sequence_mask\n",
    "    else:\n",
    "        mask = padding_mask | sequence_mask\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size)\n",
    "\n",
    "# 准备输入数据\n",
    "batch_size = 1\n",
    "src_seq_len = 10\n",
    "tgt_seq_len = 1\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# 构造掩码\n",
    "pad_id = 0\n",
    "src_mask = make_src_mask(src, pad_id)\n",
    "tgt_mask = make_tgt_mask(tgt, pad_id)\n",
    "memory_mask = make_memory_mask(src, pad_id)\n",
    "\n",
    "# 初始KV cache为None\n",
    "kv_cache = None\n",
    "\n",
    "# 推理过程\n",
    "for _ in range(5):\n",
    "    output, kv_cache = model(src, tgt, src_mask, tgt_mask, memory_mask, kv_cache)\n",
    "    next_token = torch.argmax(output[:, -1, :], dim=-1, keepdim=True)\n",
    "    tgt = torch.cat([tgt, next_token], dim=-1)\n",
    "    tgt_mask = make_tgt_mask(tgt, pad_id)\n",
    "\n",
    "\n",
    "print(\"Generated sequence:\", tgt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
